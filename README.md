# LLM Compatibility Checker

> **"Can my PC run this AI model?"** ‚Äî like *Can You Run It*, but for local LLMs.

Automatically detects your CPU, RAM, GPU, and VRAM, then scores every model in
the database against your hardware. Tells you what will run smoothly, what will
lag, and what will melt your laptop.

---

## Quick Start

```bash
git clone https://github.com/YOUR_USERNAME/llm-compat-checker.git
cd llm-compat-checker
pip install -r requirements.txt
python checker.py
```

Dependencies install automatically on first run if you skip the `pip install` step.

---

## Usage

```
python checker.py [options]
```

| Option | Description |
|---|---|
| *(no options)* | Run a full compatibility check |
| `--update` | Fetch the latest models from HuggingFace |
| `--filter <use_case>` | Filter by: `general`, `coding`, `reasoning`, `vision`, `embedding` |
| `--runnable` | Only show models your hardware can actually run |
| `--min-params <n>` | Only show models ‚â• nB parameters |
| `--max-params <n>` | Only show models ‚â§ nB parameters |
| `--export <file.json>` | Save results to a JSON file |
| `--no-legend` | Skip the legend at the bottom |

**Examples:**

```bash
python checker.py --filter coding --runnable     # Runnable code models only
python checker.py --min-params 7 --max-params 13 # 7B‚Äì13B range
python checker.py --export my_results.json       # Save to JSON
python scripts/scrape_hf_models.py --search qwen # Fetch only Qwen models
```

---

## What It Detects

**Hardware**
- CPU model, core count, and frequency
- RAM total, free, and usage percentage
- GPU name, VRAM, temperature, and load (per GPU)
- Multi-GPU: aggregates VRAM across all GPUs
- Apple Silicon: detects unified memory (RAM = VRAM)
- Storage type: SSD/NVMe vs HDD
- Form factor: laptop vs desktop
- Acceleration backend: CUDA / Metal / ROCm / CPU

**GPU detection methods (in priority order)**

| Platform | Method |
|---|---|
| NVIDIA (all OS) | `GPUtil` library, then `nvidia-smi` |
| AMD (Linux) | `rocm-smi` |
| Apple Silicon | `system_profiler` + unified memory calculation |
| Windows (fallback) | `wmic win32_VideoController` |
| Linux (fallback) | `lspci` (name only, no VRAM) |

---

## Compatibility Scoring

Each model is scored against your hardware using a weighted formula:

```
Score = (RAM ratio √ó 0.40) + (VRAM ratio √ó 0.40) + (CPU score √ó 0.20)
```

On Apple Silicon, the formula shifts to `(RAM ratio √ó 0.80) + (CPU score √ó 0.20)` since unified memory means RAM and VRAM are the same pool.

| Score | Status |
|---|---|
| ‚â• 1.4 | ‚úÖ Smooth |
| 1.0 ‚Äì 1.4 | ‚ö° Acceptable |
| 0.7 ‚Äì 1.0 | ‚ö†Ô∏è Lag Likely |
| < 0.6 RAM, or no GPU | üê¢ CPU Only |
| < 0.6 RAM | ‚ùå Won't Run |

**MoE models** (Mixtral, DeepSeek-V3, etc.) load the full weight matrix but only activate ~13% of parameters per token. The VRAM requirement is reduced by 30% to reflect this.

---

## Keeping the Model Database Fresh

The tool ships with `models_builtin.json` ‚Äî a committed database of 35+ popular models that works offline with no setup.

To expand it with live data from HuggingFace:

```bash
# Quick update (built into the main tool)
python checker.py --update

# Advanced scraper with more control
python scripts/scrape_hf_models.py
python scripts/scrape_hf_models.py --search deepseek
python scripts/scrape_hf_models.py --limit 30 --delay 0.5
```

The HuggingFace metadata API is free to use for this purpose. No API key is required. Rate limits are respected automatically.

**Database load priority:**
1. `models_db.json` ‚Äî if it exists from a previous `--update` run
2. `models_builtin.json` ‚Äî the committed fallback, always present

---

## Adding Custom Models

To add your own models, edit `models_builtin.json` ‚Äî no Python knowledge needed:

```json
{
  "name":      "My Custom Model 7B",
  "provider":  "My Lab",
  "family":    "Custom",
  "params":    7,
  "context":   32768,
  "ram":       16,
  "vram":      6,
  "quants":    ["Q4_K_M", "Q5_K_M", "Q8_0"],
  "use_case":  "general",
  "tags":      ["chat"],
  "is_moe":    false
}
```

Add the entry to the `"models"` array and it will appear in the next run.

---

## Project Structure

```
llm-compat-checker/
‚îú‚îÄ‚îÄ checker.py              Main script ‚Äî run this
‚îú‚îÄ‚îÄ models_builtin.json     Committed model database (edit to add custom models)
‚îú‚îÄ‚îÄ models_db.json          Auto-generated by --update (gitignored)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ scrape_hf_models.py Standalone HuggingFace scraper with extra options
```

---

## Built-in Models

| Family | Sizes |
|---|---|
| LLaMA 3 / 3.1 / 3.2 (Meta) | 1B, 3B, 8B, 70B, Vision 11B |
| Mistral / Mixtral | 7B, 12B, MoE 8x7B, MoE 8x22B |
| Gemma 2 (Google) | 2B, 9B, 27B |
| Phi-3 / Phi-4 (Microsoft) | 3.8B, 14B |
| Qwen2.5 (Alibaba) | 1.5B ‚Üí 72B, Coder, Vision |
| DeepSeek-R1 | 7B, 32B, 70B |
| CodeLlama (Meta) | 7B, 13B, 34B |
| StarCoder2 | 7B |
| Embeddings | nomic-embed, mxbai-embed |

Run `--update` to expand to 45+ models from HuggingFace.

---

## Contributing

Pull requests are welcome. Some ideas for contributions:

- Ollama integration ‚Äî list and check locally installed models
- LM Studio model detection
- Disk space check ‚Äî estimate GGUF download size
- Estimated power draw
- CSV export option
- Weekly model DB update via GitHub Actions

---

## License

MIT ‚Äî free to use, modify, and distribute.

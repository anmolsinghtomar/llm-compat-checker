"""
LLM Compatibility Checker
=========================
Automatically detects your hardware (CPU, RAM, GPU, VRAM) and tells you
which local AI models you can run, how fast they'll be, and whether your
laptop is going to overheat trying.

Usage:
    python checker.py                        # Run full compatibility check
    python checker.py --update               # Fetch latest models from HuggingFace
    python checker.py --filter coding        # Show only coding models
    python checker.py --filter reasoning     # Show only reasoning models
    python checker.py --runnable             # Hide models that won't run
    python checker.py --min-params 7         # Show models >= 7B only
    python checker.py --max-params 13        # Show models <= 13B only
    python checker.py --export results.json  # Save results to a JSON file

Requirements:
    pip install rich psutil GPUtil
    (auto-installed on first run if missing)
"""

import platform
import subprocess
import sys
import os
import json
import argparse
import time
import urllib.request
import urllib.error
from pathlib import Path
from datetime import datetime


# ---------------------------------------------------------------------------
# Auto-install missing packages on first run
# ---------------------------------------------------------------------------

def _install(*packages):
    """Install pip packages, with a fallback for system-managed environments."""
    try:
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", *packages],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
    except subprocess.CalledProcessError:
        # Fallback for Debian/Ubuntu systems that use externally-managed Python
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", *packages, "--break-system-packages"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )


try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
    from rich.text import Text
    from rich import box
    from rich.columns import Columns
    from rich.rule import Rule
    from rich.align import Align
except ImportError:
    print("Installing required packages (first run only)...")
    _install("rich", "psutil", "GPUtil")
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
    from rich.text import Text
    from rich import box
    from rich.columns import Columns
    from rich.rule import Rule
    from rich.align import Align

try:
    import psutil
except ImportError:
    _install("psutil")
    import psutil


console = Console()


# ---------------------------------------------------------------------------
# File paths
# ---------------------------------------------------------------------------

SCRIPT_DIR   = Path(__file__).parent
DB_PATH      = SCRIPT_DIR / "models_db.json"       # Auto-generated by --update (gitignored)
BUILTIN_PATH = SCRIPT_DIR / "models_builtin.json"  # Committed fallback database


# ---------------------------------------------------------------------------
# Quantization memory multipliers
#
# These represent what fraction of the full FP16 model size each
# quantization level uses in practice. For example, Q4_K_M uses roughly
# 55% of the memory that an unquantized (FP16) model would need.
# ---------------------------------------------------------------------------

QUANT_MEMORY_FRACTION = {
    "Q8_0":   1.00,   # Near-lossless, highest quality
    "Q6_K":   0.75,
    "Q5_K_M": 0.65,
    "Q5_K_S": 0.62,
    "Q4_K_M": 0.55,   # Best balance of quality vs size (recommended default)
    "Q4_K_S": 0.52,
    "Q3_K_M": 0.45,
    "Q3_K_S": 0.42,
    "Q2_K":   0.35,   # Smallest, noticeable quality loss
}


# ---------------------------------------------------------------------------
# HuggingFace model list (used by --update)
#
# Each entry is: (hf_repo_id, display_name, provider, family, use_case, tags)
# To add more models to the --update fetch, just add a row here.
# ---------------------------------------------------------------------------

HF_FETCH_LIST = [
    # Meta LLaMA
    ("meta-llama/Llama-3.1-8B-Instruct",          "LLaMA 3.1 8B Instruct",     "Meta",       "LLaMA",     "general",   ["chat"]),
    ("meta-llama/Llama-3.1-70B-Instruct",         "LLaMA 3.1 70B Instruct",    "Meta",       "LLaMA",     "general",   ["chat"]),
    ("meta-llama/Llama-3.2-1B-Instruct",          "LLaMA 3.2 1B Instruct",     "Meta",       "LLaMA",     "general",   ["chat"]),
    ("meta-llama/Llama-3.2-3B-Instruct",          "LLaMA 3.2 3B Instruct",     "Meta",       "LLaMA",     "general",   ["chat"]),
    ("meta-llama/Llama-3.2-11B-Vision-Instruct",  "LLaMA 3.2 Vision 11B",      "Meta",       "LLaMA",     "vision",    ["vision", "multimodal"]),
    ("meta-llama/Llama-3.3-70B-Instruct",         "LLaMA 3.3 70B Instruct",    "Meta",       "LLaMA",     "general",   ["chat"]),

    # Mistral / Mixtral
    ("mistralai/Mistral-7B-Instruct-v0.3",        "Mistral 7B Instruct",       "Mistral AI", "Mistral",   "general",   ["chat"]),
    ("mistralai/Mistral-Nemo-Instruct-2407",      "Mistral Nemo 12B",          "Mistral AI", "Mistral",   "general",   ["chat"]),
    ("mistralai/Mixtral-8x7B-Instruct-v0.1",      "Mixtral 8x7B Instruct",     "Mistral AI", "Mixtral",   "general",   ["chat", "moe"]),
    ("mistralai/Mixtral-8x22B-Instruct-v0.1",     "Mixtral 8x22B Instruct",    "Mistral AI", "Mixtral",   "general",   ["chat", "moe"]),

    # Google Gemma
    ("google/gemma-2-2b-it",                      "Gemma 2 2B",                "Google",     "Gemma",     "general",   ["chat"]),
    ("google/gemma-2-9b-it",                      "Gemma 2 9B",                "Google",     "Gemma",     "general",   ["chat"]),
    ("google/gemma-2-27b-it",                     "Gemma 2 27B",               "Google",     "Gemma",     "general",   ["chat"]),

    # Microsoft Phi
    ("microsoft/Phi-3.5-mini-instruct",           "Phi-3.5 Mini 3.8B",         "Microsoft",  "Phi",       "coding",    ["code", "chat"]),
    ("microsoft/Phi-3-medium-128k-instruct",      "Phi-3 Medium 14B",          "Microsoft",  "Phi",       "coding",    ["code", "reasoning"]),
    ("microsoft/phi-4",                           "Phi-4 14B",                 "Microsoft",  "Phi",       "coding",    ["code", "reasoning"]),

    # Alibaba Qwen
    ("Qwen/Qwen2.5-0.5B-Instruct",               "Qwen2.5 0.5B",              "Alibaba",    "Qwen",      "general",   ["chat", "multilingual"]),
    ("Qwen/Qwen2.5-1.5B-Instruct",               "Qwen2.5 1.5B",              "Alibaba",    "Qwen",      "general",   ["chat", "multilingual"]),
    ("Qwen/Qwen2.5-3B-Instruct",                 "Qwen2.5 3B",                "Alibaba",    "Qwen",      "general",   ["chat", "multilingual"]),
    ("Qwen/Qwen2.5-7B-Instruct",                 "Qwen2.5 7B",                "Alibaba",    "Qwen",      "general",   ["chat", "multilingual"]),
    ("Qwen/Qwen2.5-14B-Instruct",                "Qwen2.5 14B",               "Alibaba",    "Qwen",      "general",   ["chat", "multilingual"]),
    ("Qwen/Qwen2.5-32B-Instruct",                "Qwen2.5 32B",               "Alibaba",    "Qwen",      "general",   ["chat", "multilingual"]),
    ("Qwen/Qwen2.5-72B-Instruct",                "Qwen2.5 72B",               "Alibaba",    "Qwen",      "general",   ["chat", "multilingual"]),
    ("Qwen/Qwen2.5-Coder-7B-Instruct",           "Qwen2.5-Coder 7B",          "Alibaba",    "Qwen",      "coding",    ["code"]),
    ("Qwen/Qwen2.5-Coder-32B-Instruct",          "Qwen2.5-Coder 32B",         "Alibaba",    "Qwen",      "coding",    ["code"]),
    ("Qwen/QwQ-32B",                             "QwQ 32B",                   "Alibaba",    "Qwen",      "reasoning", ["reasoning", "math"]),
    ("Qwen/Qwen2-VL-7B-Instruct",               "Qwen2-VL 7B",               "Alibaba",    "Qwen",      "vision",    ["vision", "multimodal"]),

    # DeepSeek
    ("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B","DeepSeek-R1 Distill 1.5B",  "DeepSeek",   "DeepSeek",  "reasoning", ["reasoning", "math"]),
    ("deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",  "DeepSeek-R1 Distill 7B",    "DeepSeek",   "DeepSeek",  "reasoning", ["reasoning", "math"]),
    ("deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "DeepSeek-R1 Distill 14B",   "DeepSeek",   "DeepSeek",  "reasoning", ["reasoning", "math"]),
    ("deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "DeepSeek-R1 Distill 32B",   "DeepSeek",   "DeepSeek",  "reasoning", ["reasoning", "math"]),

    # Code models
    ("bigcode/starcoder2-7b",                    "StarCoder2 7B",              "BigCode",    "StarCoder", "coding",    ["code"]),
    ("bigcode/starcoder2-15b",                   "StarCoder2 15B",             "BigCode",    "StarCoder", "coding",    ["code"]),
    ("codellama/CodeLlama-7b-Instruct-hf",       "CodeLlama 7B",               "Meta",       "CodeLlama", "coding",    ["code"]),
    ("codellama/CodeLlama-13b-Instruct-hf",      "CodeLlama 13B",              "Meta",       "CodeLlama", "coding",    ["code"]),
    ("codellama/CodeLlama-34b-Instruct-hf",      "CodeLlama 34B",              "Meta",       "CodeLlama", "coding",    ["code"]),

    # Embedding models
    ("nomic-ai/nomic-embed-text-v1.5",           "nomic-embed-text v1.5",      "Nomic",      "Nomic",     "embedding", ["embedding"]),
    ("mixedbread-ai/mxbai-embed-large-v1",       "mxbai-embed-large v1",       "MixedBread", "BERT",      "embedding", ["embedding"]),
    ("BAAI/bge-m3",                              "BGE-M3",                     "BAAI",       "BGE",       "embedding", ["embedding", "multilingual"]),
]


# ---------------------------------------------------------------------------
# HuggingFace update functions
# ---------------------------------------------------------------------------

def _hf_fetch(repo_id):
    """
    Fetch model metadata from the HuggingFace REST API.
    Returns a dict on success, or None if the request fails.
    """
    url = f"https://huggingface.co/api/models/{repo_id}"
    try:
        request = urllib.request.Request(
            url,
            headers={"User-Agent": "llm-compat-checker/2.0"},
        )
        with urllib.request.urlopen(request, timeout=8) as response:
            return json.loads(response.read().decode())
    except Exception:
        return None


def _extract_param_count(data, repo_id, display_name):
    """
    Try to figure out how many parameters a model has from its HF metadata.
    We try three methods in order:
      1. safetensors total (most accurate)
      2. Tags that look like "7B" or "70billion"
      3. Guessing from the repo name / display name
    """
    # Method 1: safetensors reports exact parameter count
    try:
        total = data.get("safetensors", {}).get("total", 0)
        if total and total > 0:
            return round(total / 1e9, 1)
    except Exception:
        pass

    # Method 2: look for size tags like "7b", "70billion"
    try:
        for tag in data.get("tags", []):
            t = tag.lower()
            for suffix in ["b", "billion"]:
                if t.endswith(suffix):
                    try:
                        return float(t.replace(suffix, "").strip())
                    except ValueError:
                        pass
    except Exception:
        pass

    # Method 3: infer from the name (e.g. "llama-3-70b" ‚Üí 70)
    combined = (repo_id + " " + display_name).lower().replace(" ", "")
    for size in [405, 141, 70, 72, 67, 34, 32, 27, 22, 14, 13, 12, 11, 9, 8, 7, 4, 3, 2, 1.5, 1.1, 0.5]:
        if f"{size}b" in combined or f"-{size}-" in combined:
            return float(size)

    return None


def _build_model_entry(repo_id, display_name, provider, family, use_case, tags, hf_data):
    """
    Convert a HuggingFace API response into the model dict format used
    by this tool. Returns None if we can't determine the parameter count.
    """
    params = _extract_param_count(hf_data, repo_id, display_name)
    if params is None:
        return None

    is_moe = "moe" in tags or any(k in repo_id.lower() for k in ["8x", "moe", "mixture"])

    # Memory estimates based on FP16 size with overhead factors:
    #   RAM  = params √ó 2 bytes √ó 1.2 (OS + context overhead)
    #   VRAM = params √ó 2 bytes √ó 1.1 (slightly less overhead on GPU)
    ram_gb  = int(max(round(params * 2.0 * 1.2), 4))
    vram_gb = int(max(round(params * 2.0 * 1.1), 2))

    # Try to get context window length from model card
    context = 4096
    try:
        card = hf_data.get("cardData") or {}
        for key in ("context_length", "max_position_embeddings", "max_seq_len"):
            val = card.get(key)
            if val:
                context = int(val)
                break
    except Exception:
        pass

    # Assign available quantizations based on model size
    if params <= 3:
        quants = ["Q4_K_M", "Q5_K_M", "Q8_0"]
    elif params <= 7:
        quants = ["Q3_K_M", "Q4_K_M", "Q5_K_M", "Q8_0"]
    elif params <= 13:
        quants = ["Q3_K_M", "Q4_K_M", "Q5_K_M"]
    elif params <= 34:
        quants = ["Q3_K_M", "Q4_K_M"]
    else:
        quants = ["Q2_K", "Q3_K_M", "Q4_K_M"]

    return {
        "name":      display_name,
        "provider":  provider,
        "family":    family,
        "params":    params,
        "context":   context,
        "ram":       ram_gb,
        "vram":      vram_gb,
        "quants":    quants,
        "use_case":  use_case,
        "tags":      tags,
        "is_moe":    is_moe,
        "hf_repo":   repo_id,
        "likes":     hf_data.get("likes", 0),
        "downloads": hf_data.get("downloads", 0),
    }


def update_model_db():
    """
    Fetch metadata for all models in HF_FETCH_LIST and merge them with the
    built-in database. Saves the result to models_db.json.
    """
    console.print()
    console.print(Rule("[bold cyan]Updating Model Database from HuggingFace", style="cyan"))
    console.print()

    # Start with the built-in models as a base
    try:
        base_models = json.loads(BUILTIN_PATH.read_text()).get("models", [])
    except Exception:
        base_models = []

    updated_models = list(base_models)
    success_count  = 0
    failed_count   = 0

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(bar_width=35),
        TaskProgressColumn(),
        console=console,
    ) as progress:
        task = progress.add_task("Fetching...", total=len(HF_FETCH_LIST))

        for repo_id, display_name, provider, family, use_case, tags in HF_FETCH_LIST:
            progress.update(task, description=f"[cyan]{display_name[:45]}[/cyan]")

            hf_data = _hf_fetch(repo_id)
            if hf_data:
                model = _build_model_entry(repo_id, display_name, provider, family, use_case, tags, hf_data)
                if model:
                    # Update existing entry or append new one
                    existing = [i for i, m in enumerate(updated_models) if m["name"] == model["name"]]
                    if existing:
                        updated_models[existing[0]] = model
                    else:
                        updated_models.append(model)
                    success_count += 1
                else:
                    failed_count += 1
            else:
                failed_count += 1

            progress.advance(task, 1)
            time.sleep(0.2)  # Be respectful of HuggingFace rate limits

    db = {
        "updated_at":  datetime.now().isoformat(),
        "model_count": len(updated_models),
        "source":      "huggingface_api",
        "models":      updated_models,
    }
    DB_PATH.write_text(json.dumps(db, indent=2))

    console.print()
    console.print(f"  [green]Fetched {success_count} models from HuggingFace[/green]")
    console.print(f"  [cyan]{len(updated_models)} total models in database[/cyan]")
    if failed_count:
        console.print(f"  [yellow]{failed_count} models skipped (private repo or API limit)[/yellow]")
    console.print(f"  [dim]Saved to: {DB_PATH}[/dim]")
    console.print()


# ---------------------------------------------------------------------------
# Model database loader
# ---------------------------------------------------------------------------

def load_models():
    """
    Load the model list with the following priority:
      1. models_db.json  ‚Äî generated by running with --update
      2. models_builtin.json ‚Äî the committed fallback (always present)

    If neither file exists, the program exits with a clear error message.
    """
    # Priority 1: live database from --update
    if DB_PATH.exists():
        try:
            data    = json.loads(DB_PATH.read_text())
            models  = data.get("models", [])
            updated = data.get("updated_at", "unknown")[:10]
            console.print(f"  [dim]Loaded {len(models)} models  "
                          f"[green](models_db.json ¬∑ updated {updated})[/green][/dim]")
            return models
        except Exception:
            pass  # Fall through to built-in

    # Priority 2: committed built-in JSON
    if BUILTIN_PATH.exists():
        try:
            data   = json.loads(BUILTIN_PATH.read_text())
            models = data.get("models", [])
            console.print(f"  [dim]Loaded {len(models)} models  "
                          f"[cyan](models_builtin.json)[/cyan]"
                          f"  ¬∑ run [bold]--update[/bold] to fetch latest from HuggingFace[/dim]")
            return models
        except Exception:
            pass

    # Neither file found
    console.print("[bold red]No model database found![/bold red]")
    console.print("Make sure [bold]models_builtin.json[/bold] is in the same folder as checker.py,")
    console.print("or run [bold]python checker.py --update[/bold] to download from HuggingFace.")
    sys.exit(1)


# ---------------------------------------------------------------------------
# Hardware detection
# ---------------------------------------------------------------------------

def detect_backend(info):
    """
    Determine which AI acceleration backend is available on this machine.
    Returns a string like "CUDA", "Metal (Apple Silicon)", "ROCm", etc.
    """
    gpus    = info.get("gpus", [])
    os_name = info.get("os", "")

    if gpus:
        vendor = gpus[0].get("vendor", "").upper()
        if "NVIDIA" in vendor:
            return "CUDA"
        if "AMD" in vendor or "RADEON" in vendor:
            return "ROCm"
        if "APPLE" in vendor or "METAL" in vendor:
            return "Metal (Apple Silicon)"
        if "INTEL" in vendor:
            return "SYCL / OpenCL"

    # Check for Apple Silicon even without a discrete GPU entry
    if os_name == "Darwin":
        try:
            brand = subprocess.check_output(
                ["sysctl", "-n", "machdep.cpu.brand_string"],
                stderr=subprocess.DEVNULL,
            ).decode().strip()
            if "Apple" in brand:
                return "Metal (Apple Silicon)"
        except Exception:
            pass

    # CPU-only fallback
    machine = info.get("machine", "")
    if "arm" in machine.lower() or "aarch64" in machine.lower():
        return "CPU (ARM)"
    return "CPU (x86)"


def _get_apple_silicon_vram(info):
    """
    On Apple Silicon, RAM and VRAM are unified ‚Äî the same physical memory
    is shared between CPU and GPU. We treat 75% of system RAM as usable
    for model inference (the rest is reserved for the OS and other apps).
    Returns 0.0 on non-Apple-Silicon machines.
    """
    if info.get("os") != "Darwin":
        return 0.0
    try:
        brand = subprocess.check_output(
            ["sysctl", "-n", "machdep.cpu.brand_string"],
            stderr=subprocess.DEVNULL,
        ).decode().strip()
        if "Apple" in brand:
            return round(info.get("ram_total_gb", 0) * 0.75, 1)
    except Exception:
        pass
    return 0.0


def _detect_gpus(info):
    """
    Detect GPU(s) using multiple methods depending on platform and vendor.
    Populates info["gpus"] and info["total_vram_gb"].

    Detection order:
      1. GPUtil Python library (NVIDIA, cross-platform)
      2. nvidia-smi CLI (NVIDIA, cross-platform)
      3. rocm-smi CLI (AMD, Linux)
      4. system_profiler (Apple, macOS)
      5. wmic (Windows fallback)
      6. lspci (Linux fallback ‚Äî name only, no VRAM)
    """
    gpus = []

    # --- Method 1: GPUtil (NVIDIA) ---
    try:
        import GPUtil
        for g in GPUtil.getGPUs():
            gpus.append({
                "name":   g.name,
                "vram_gb": round(g.memoryTotal / 1024, 1),
                "vendor": "NVIDIA",
                "driver": g.driver,
                "load":   g.load,
                "temp":   g.temperature,
            })
    except Exception:
        pass

    # --- Method 2: nvidia-smi CLI (NVIDIA) ---
    if not gpus:
        try:
            output = subprocess.check_output(
                ["nvidia-smi",
                 "--query-gpu=name,memory.total,temperature.gpu,utilization.gpu",
                 "--format=csv,noheader,nounits"],
                stderr=subprocess.DEVNULL,
                timeout=5,
            ).decode().strip()
            for line in output.splitlines():
                parts = [p.strip() for p in line.split(",")]
                if len(parts) >= 2:
                    vram = round(int(parts[1]) / 1024, 1)
                    temp = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else None
                    load = float(parts[3]) / 100 if len(parts) > 3 else None
                    gpus.append({
                        "name":    parts[0],
                        "vram_gb": vram,
                        "vendor":  "NVIDIA",
                        "driver":  "N/A",
                        "load":    load,
                        "temp":    temp,
                    })
        except Exception:
            pass

    # --- Method 3: rocm-smi (AMD, Linux) ---
    if not gpus:
        try:
            import re
            output = subprocess.check_output(
                ["rocm-smi", "--showproductname", "--showmeminfo", "vram"],
                stderr=subprocess.DEVNULL,
                timeout=5,
            ).decode()
            names  = re.findall(r"GPU\[.*?\].*?:\s*(.+)", output)
            totals = re.findall(r"Total Memory.*?:\s*(\d+)", output)
            for i, name in enumerate(names):
                vram = round(int(totals[i]) / (1024 ** 3), 1) if i < len(totals) else 0
                gpus.append({
                    "name":    name.strip(),
                    "vram_gb": vram,
                    "vendor":  "AMD",
                    "driver":  "ROCm",
                    "load":    None,
                    "temp":    None,
                })
        except Exception:
            pass

    # --- Method 4: system_profiler (Apple, macOS) ---
    if not gpus and platform.system() == "Darwin":
        try:
            output = subprocess.check_output(
                ["system_profiler", "SPDisplaysDataType", "-json"],
                stderr=subprocess.DEVNULL,
                timeout=8,
            ).decode()
            data = json.loads(output)
            for display in data.get("SPDisplaysDataType", []):
                name     = display.get("sppci_model", "Apple GPU")
                vram_str = display.get("spdisplays_vram", "0 MB")
                try:
                    vram_val = float(vram_str.split()[0])
                    unit     = vram_str.split()[1].upper() if len(vram_str.split()) > 1 else "MB"
                    vram_gb  = round(vram_val / 1024 if unit == "MB" else vram_val, 1)
                except Exception:
                    vram_gb = 0
                vendor = ("NVIDIA" if "nvidia" in name.lower()
                          else "AMD" if "amd" in name.lower()
                          else "Apple")
                gpus.append({
                    "name":    name,
                    "vram_gb": vram_gb,
                    "vendor":  vendor,
                    "driver":  "Metal",
                    "load":    None,
                    "temp":    None,
                })
        except Exception:
            pass

    # --- Method 5: wmic (Windows fallback) ---
    if not gpus and platform.system() == "Windows":
        try:
            output = subprocess.check_output(
                ["wmic", "path", "win32_VideoController",
                 "get", "Name,AdapterRAM,DriverVersion", "/format:csv"],
                stderr=subprocess.DEVNULL,
                timeout=5,
            ).decode()
            for line in output.splitlines():
                parts = line.strip().split(",")
                if len(parts) >= 3 and parts[2].strip() not in ("", "Name"):
                    try:
                        vram = round(int(parts[1].strip()) / (1024 ** 3), 1)
                        name = parts[2].strip()
                        vendor = ("NVIDIA" if "NVIDIA" in name
                                  else "AMD" if ("AMD" in name or "Radeon" in name)
                                  else "Intel / Other")
                        gpus.append({
                            "name":    name,
                            "vram_gb": vram,
                            "vendor":  vendor,
                            "driver":  parts[3].strip() if len(parts) > 3 else "N/A",
                            "load":    None,
                            "temp":    None,
                        })
                    except Exception:
                        pass
        except Exception:
            pass

    # --- Method 6: lspci (Linux fallback ‚Äî names only, no VRAM) ---
    if not gpus and platform.system() == "Linux":
        try:
            output = subprocess.check_output(
                ["lspci"], stderr=subprocess.DEVNULL, timeout=5
            ).decode()
            for line in output.splitlines():
                if any(k in line for k in ("VGA", "3D controller", "Display")):
                    name   = line.split(":")[-1].strip()
                    vendor = ("NVIDIA" if "NVIDIA" in name
                              else "AMD" if ("AMD" in name or "Radeon" in name)
                              else "Intel / Other")
                    gpus.append({
                        "name":    name,
                        "vram_gb": 0,  # lspci can't report VRAM
                        "vendor":  vendor,
                        "driver":  "N/A",
                        "load":    None,
                        "temp":    None,
                    })
        except Exception:
            pass

    # Deduplicate by name
    seen, unique = set(), []
    for g in gpus:
        key = g["name"].lower().strip()
        if key and key not in seen:
            seen.add(key)
            unique.append(g)

    info["gpus"]          = unique
    info["total_vram_gb"] = sum(g["vram_gb"] for g in unique)


def _detect_storage_type():
    """
    Return "SSD/NVMe", "HDD", or "Unknown" based on the primary storage device.
    Matters because model load times are much faster from SSDs.
    """
    try:
        if platform.system() == "Windows":
            output = subprocess.check_output(
                ["powershell", "-command",
                 "Get-PhysicalDisk | Select-Object MediaType | Format-Table -HideTableHeaders"],
                stderr=subprocess.DEVNULL,
                timeout=5,
            ).decode()
            if "SSD" in output or "NVMe" in output:
                return "SSD / NVMe"
            if "HDD" in output:
                return "HDD"

        elif platform.system() == "Linux":
            output = subprocess.check_output(
                ["lsblk", "-d", "-o", "NAME,ROTA"],
                stderr=subprocess.DEVNULL,
                timeout=5,
            ).decode()
            # ROTA=0 means non-rotational (SSD/NVMe), ROTA=1 means HDD
            if " 0" in output:
                return "SSD / NVMe"
            return "HDD"

        elif platform.system() == "Darwin":
            output = subprocess.check_output(
                ["system_profiler", "SPStorageDataType"],
                stderr=subprocess.DEVNULL,
                timeout=5,
            ).decode()
            if any(k in output for k in ("SSD", "Flash", "NVMe")):
                return "SSD / NVMe"
            return "HDD"

    except Exception:
        pass

    return "Unknown"


def _detect_laptop():
    """
    Return True if the machine appears to be a laptop.
    Laptops are flagged because sustained AI inference can cause thermal
    throttling and battery drain.
    """
    try:
        if platform.system() == "Windows":
            output = subprocess.check_output(
                ["wmic", "systemenclosure", "get", "chassistypes"],
                stderr=subprocess.DEVNULL,
                timeout=5,
            ).decode()
            # Chassis type codes 8, 9, 10, 14 = laptop/notebook/tablet
            for code in ["8", "9", "10", "14"]:
                if code in output:
                    return True

        elif platform.system() == "Linux":
            chassis_file = "/sys/class/dmi/id/chassis_type"
            if os.path.exists(chassis_file):
                with open(chassis_file) as f:
                    if f.read().strip() in ["8", "9", "10", "14"]:
                        return True
            # Battery presence is a reliable laptop indicator
            if os.path.exists("/sys/class/power_supply/BAT0"):
                return True

        elif platform.system() == "Darwin":
            output = subprocess.check_output(
                ["system_profiler", "SPHardwareDataType"],
                stderr=subprocess.DEVNULL,
                timeout=5,
            ).decode()
            if "MacBook" in output:
                return True

    except Exception:
        pass

    return False


def get_system_info():
    """
    Collect all hardware information needed for compatibility scoring.
    Returns a dict with CPU, RAM, GPU, storage, and platform details.
    """
    info = {
        "os":         platform.system(),
        "os_version": platform.version(),
        "machine":    platform.machine(),
    }

    # CPU name ‚Äî platform.processor() is unreliable on Linux/macOS
    cpu_name = platform.processor() or ""
    if not cpu_name.strip():
        try:
            if info["os"] == "Darwin":
                cpu_name = subprocess.check_output(
                    ["sysctl", "-n", "machdep.cpu.brand_string"],
                    stderr=subprocess.DEVNULL,
                ).decode().strip()
            elif info["os"] == "Linux":
                for line in subprocess.check_output(
                    ["lscpu"], stderr=subprocess.DEVNULL
                ).decode().splitlines():
                    if "Model name" in line:
                        cpu_name = line.split(":")[1].strip()
                        break
        except Exception:
            pass

    info["cpu_name"]           = cpu_name or "Unknown CPU"
    info["cpu_cores_physical"] = psutil.cpu_count(logical=False) or 0
    info["cpu_cores_logical"]  = psutil.cpu_count(logical=True) or 0

    try:
        freq = psutil.cpu_freq()
        info["cpu_freq_ghz"] = round(freq.max / 1000, 2) if freq and freq.max else None
    except Exception:
        info["cpu_freq_ghz"] = None

    # RAM
    ram = psutil.virtual_memory()
    info["ram_total_gb"]     = round(ram.total / (1024 ** 3), 1)
    info["ram_available_gb"] = round(ram.available / (1024 ** 3), 1)
    info["ram_used_pct"]     = ram.percent

    # GPU
    info["gpus"]             = []
    info["total_vram_gb"]    = 0
    info["is_apple_silicon"] = False
    info["multi_gpu"]        = False
    _detect_gpus(info)

    # Apple Silicon override: unified memory = no discrete GPU, but RAM IS VRAM
    apple_vram = _get_apple_silicon_vram(info)
    if apple_vram > 0 and not info["gpus"]:
        info["gpus"] = [{
            "name":    info["cpu_name"],
            "vram_gb": apple_vram,
            "vendor":  "Apple (Unified Memory)",
            "driver":  "Metal",
            "load":    None,
            "temp":    None,
        }]
        info["total_vram_gb"]    = apple_vram
        info["is_apple_silicon"] = True

    # Multi-GPU: sum all VRAM
    if len(info["gpus"]) > 1:
        info["total_vram_gb"] = sum(g["vram_gb"] for g in info["gpus"])
        info["multi_gpu"]     = True

    info["backend"]      = detect_backend(info)
    info["storage_type"] = _detect_storage_type()
    info["is_laptop"]    = _detect_laptop()

    return info


# ---------------------------------------------------------------------------
# Compatibility scoring
# ---------------------------------------------------------------------------

def _recommend_quant(model, info):
    """
    Return the highest-quality GGUF quantization the user's hardware can
    handle for a given model. Higher quant = better quality but more VRAM/RAM.
    """
    vram   = info["total_vram_gb"]
    ram    = info["ram_total_gb"]
    quants = model.get("quants", [])
    if not quants:
        return "N/A"

    base_vram = model["vram"]

    # Try from highest quality to lowest, return first that fits
    for quant in reversed(quants):
        fraction   = QUANT_MEMORY_FRACTION.get(quant, 0.55)
        needed_vram = base_vram * fraction
        needed_ram  = base_vram * fraction * 1.2
        if (vram > 0 and vram >= needed_vram) or (vram == 0 and ram >= needed_ram):
            return quant

    return quants[-1]  # Return lowest if nothing fits well


def estimate_speed(model, info, status):
    """
    Estimate rough tokens per second based on available hardware, the
    selected quantization, and the inference backend.

    These are ballpark estimates for planning purposes ‚Äî actual performance
    depends heavily on the specific hardware and software stack.
    """
    if status == "impossible":
        return "‚Äî"

    params  = model["params"]
    vram    = info["total_vram_gb"]
    backend = info.get("backend", "CPU")
    quant   = _recommend_quant(model, info)
    q_mult  = QUANT_MEMORY_FRACTION.get(quant, 0.55)

    if "CUDA" in backend:
        # NVIDIA GPU: fast parallel inference
        base = max(1.0, (vram / max(params, 0.1)) * 15 * q_mult)
    elif "Metal" in backend:
        # Apple Silicon: efficient unified memory access
        base = max(1.0, (vram / max(params, 0.1)) * 12 * q_mult)
    elif "ROCm" in backend:
        # AMD GPU
        base = max(1.0, (vram / max(params, 0.1)) * 10 * q_mult)
    else:
        # CPU-only: much slower
        cores = info.get("cpu_cores_logical", 4)
        base  = max(0.5, (cores / max(params, 0.1)) * 3 * q_mult)

    # CPU-only fallback is dramatically slower than GPU inference
    if status == "cpu_only":
        base *= 0.15

    base = round(base, 1)

    if base < 1:
        return f"~{base} t/s"
    elif base < 20:
        return f"~{int(base)} t/s"
    else:
        return f"~{int(base)} t/s ‚ö°"


def score_model(model, info):
    """
    Score a model against the current hardware and return a status + details.

    The composite score is a weighted combination of:
      - RAM ratio (40%)  ‚Äî how much RAM do we have vs what's needed
      - VRAM ratio (40%) ‚Äî same for GPU memory
      - CPU score (20%)  ‚Äî based on logical core count

    Apple Silicon gets a modified formula since RAM = VRAM there.

    Returns: (score: float, status: str, details: dict)
    """
    ram_total  = info["ram_total_gb"]
    vram_total = info["total_vram_gb"]
    req_ram    = model["ram"]
    req_vram   = model["vram"]
    is_moe     = model.get("is_moe", False)
    is_apple   = info.get("is_apple_silicon", False)

    # MoE models load the full weight matrix but only activate ~13% per pass.
    # Reduce VRAM requirement by 30% to account for this.
    if is_moe:
        req_vram = round(req_vram * 0.70, 1)

    ram_ratio  = ram_total / req_ram   if req_ram  else 2.0
    vram_ratio = vram_total / req_vram if (req_vram and vram_total > 0) else 0.0
    cpu_score  = min((info.get("cpu_cores_logical") or 4) / 8, 1.5)

    # Weighted composite score
    if is_apple:
        # Unified memory: RAM IS VRAM, so weight RAM much higher
        score = (min(ram_ratio, 2.0) * 0.80) + (cpu_score * 0.20)
    elif vram_total == 0:
        # No GPU: everything falls on CPU + RAM
        score = (min(ram_ratio, 2.0) * 0.70) + (cpu_score * 0.30)
    else:
        score = (min(ram_ratio,  2.0) * 0.40 +
                 min(vram_ratio, 2.0) * 0.40 +
                 cpu_score             * 0.20)

    # Determine status from score and memory availability
    if ram_total < req_ram * 0.6:
        status = "impossible"
    elif vram_total == 0 or vram_total < req_vram:
        status = "cpu_only" if ram_total >= req_ram else "impossible"
    elif score >= 1.4:
        status = "smooth"
    elif score >= 1.0:
        status = "acceptable"
    elif score >= 0.7:
        status = "lag"
    else:
        status = "impossible"

    # Laptops under sustained load risk thermal throttling
    thermal_risk = info["is_laptop"] and status in ("acceptable", "lag", "cpu_only")

    details = {
        "ram_ratio":         ram_ratio,
        "vram_ratio":        vram_ratio,
        "gpu_fallback":      vram_total < req_vram,
        "thermal_risk":      thermal_risk,
        "recommended_quant": _recommend_quant(model, info),
        "tokens_per_sec":    estimate_speed(model, info, status),
        "is_moe":            is_moe,
    }

    return score, status, details


# ---------------------------------------------------------------------------
# Terminal UI rendering
# ---------------------------------------------------------------------------

# Label and color for each status level
STATUS_DISPLAY = {
    "smooth":     ("‚úÖ Smooth",       "bold green"),
    "acceptable": ("‚ö° Acceptable",   "bold yellow"),
    "lag":        ("‚ö†Ô∏è  Lag Likely",   "bold dark_orange"),
    "cpu_only":   ("üê¢ CPU Only",     "bold orange1"),
    "impossible": ("‚ùå Won't Run",    "bold red"),
}

# Color for each acceleration backend
BACKEND_COLOR = {
    "CUDA":                  "green",
    "Metal (Apple Silicon)": "cyan",
    "Metal":                 "cyan",
    "ROCm":                  "yellow",
    "SYCL / OpenCL":         "blue",
    "CPU (ARM)":             "dim",
    "CPU (x86)":             "dim",
}


def render_header():
    console.print()
    console.print(Align.center(Panel.fit(
        "[bold cyan]LLM Compatibility Checker[/bold cyan]  [dim]v2.0[/dim]\n"
        "[dim]Find out which AI models your PC can actually run[/dim]",
        border_style="cyan",
        padding=(1, 6),
    )))
    console.print()


def render_system_info(info):
    console.print(Rule("[bold]Your System", style="cyan"))
    console.print()

    freq_str  = f" @ {info['cpu_freq_ghz']} GHz" if info.get("cpu_freq_ghz") else ""
    ram_color = ("green" if info["ram_used_pct"] < 60
                 else "yellow" if info["ram_used_pct"] < 80
                 else "red")
    backend_color = BACKEND_COLOR.get(info.get("backend", ""), "white")

    # Left panel: CPU and memory
    cpu_text = Text()
    cpu_text.append("  CPU          ", style="dim")
    cpu_text.append(f"{info['cpu_name']}{freq_str}\n", style="bold white")
    cpu_text.append("  Cores        ", style="dim")
    cpu_text.append(f"{info['cpu_cores_physical']} physical / {info['cpu_cores_logical']} logical\n", style="white")
    cpu_text.append("  RAM Total    ", style="dim")
    cpu_text.append(f"{info['ram_total_gb']} GB\n", style="bold white")
    cpu_text.append("  RAM Free     ", style="dim")
    cpu_text.append(f"{info['ram_available_gb']} GB ", style="white")
    cpu_text.append(f"({info['ram_used_pct']}% used)\n", style=ram_color)
    cpu_text.append("  Storage      ", style="dim")
    cpu_text.append(f"{info['storage_type']}\n", style="white")
    cpu_text.append("  Form Factor  ", style="dim")
    cpu_text.append(f"{'Laptop' if info['is_laptop'] else 'Desktop'}\n", style="white")
    cpu_text.append("  OS           ", style="dim")
    cpu_text.append(f"{info['os']} ({info['machine']})\n", style="white")
    cpu_text.append("  Backend      ", style="dim")
    cpu_text.append(f"{info.get('backend', 'Unknown')}", style=f"bold {backend_color}")

    # Right panel: GPU info
    gpu_text = Text()
    if info["gpus"]:
        for i, gpu in enumerate(info["gpus"]):
            gpu_text.append(f"  GPU {i + 1}         ", style="dim")
            gpu_text.append(f"{gpu['name']}\n", style="bold white")
            gpu_text.append("  Vendor        ", style="dim")
            gpu_text.append(f"{gpu['vendor']}\n", style="white")
            gpu_text.append("  VRAM          ", style="dim")
            gpu_text.append(f"{gpu['vram_gb']} GB\n", style="bold cyan")

            if gpu.get("temp") is not None:
                temp_color = ("green" if gpu["temp"] < 70
                              else "yellow" if gpu["temp"] < 85
                              else "red")
                gpu_text.append("  Temperature   ", style="dim")
                gpu_text.append(f"{gpu['temp']}¬∞C\n", style=temp_color)

            if gpu.get("load") is not None:
                load_color = ("green" if gpu["load"] < 0.5
                              else "yellow" if gpu["load"] < 0.8
                              else "red")
                gpu_text.append("  Load          ", style="dim")
                gpu_text.append(f"{int(gpu['load'] * 100)}%\n", style=load_color)

            if i < len(info["gpus"]) - 1:
                gpu_text.append("\n")

        if info.get("multi_gpu"):
            gpu_text.append(f"\n  Total VRAM    ", style="dim")
            gpu_text.append(f"{info['total_vram_gb']} GB  (multi-GPU)", style="bold cyan")

        if info.get("is_apple_silicon"):
            gpu_text.append("\n  [dim italic](Unified memory ‚Äî RAM and VRAM are shared)[/dim italic]")
    else:
        gpu_text.append("  No dedicated GPU detected\n", style="yellow")
        gpu_text.append("  Models will run on CPU only.\n", style="dim")
        gpu_text.append("  Use llama.cpp for best CPU performance.", style="dim")

    console.print(Columns([
        Panel(cpu_text, title="[bold]CPU & Memory", border_style="blue",    padding=(0, 1)),
        Panel(gpu_text, title="[bold]GPU",          border_style="magenta", padding=(0, 1)),
    ], equal=True, expand=True))
    console.print()


def render_results_table(results):
    console.print(Rule("[bold]Model Compatibility", style="cyan"))
    console.print()

    table = Table(
        box=box.ROUNDED,
        border_style="dim",
        show_header=True,
        header_style="bold cyan",
        expand=True,
        padding=(0, 1),
    )

    table.add_column("Model",       style="white",    min_width=26, no_wrap=True)
    table.add_column("Provider",    style="dim",      min_width=12, no_wrap=True)
    table.add_column("Size",        style="dim",      justify="right", min_width=6)
    table.add_column("Context",     style="dim",      justify="right", min_width=8)
    table.add_column("RAM",         style="dim",      justify="right", min_width=6)
    table.add_column("VRAM",        style="dim",      justify="right", min_width=6)
    table.add_column("Status",      justify="center", min_width=17)
    table.add_column("Best Quant",  justify="center", style="cyan", min_width=10)
    table.add_column("Est. Speed",  justify="center", min_width=12)
    table.add_column("Thermal",     justify="center", min_width=8)

    for model, score, status, details in results:
        label, style = STATUS_DISPLAY[status]

        # Append [MoE] tag to model name if applicable
        name = model["name"]
        if details.get("is_moe"):
            name += " [dim][MoE][/dim]"

        # Format context window as "128K", "32K", "4K", etc.
        ctx = model.get("context", 0)
        ctx_str = f"{ctx // 1000}K" if ctx >= 1000 else str(ctx)

        thermal       = "Hot" if details["thermal_risk"] else "OK"
        thermal_style = "red" if details["thermal_risk"] else "green"

        table.add_row(
            name,
            model.get("provider", "‚Äî"),
            f"{model['params']}B",
            ctx_str,
            f"{model['ram']} GB",
            f"{model['vram']} GB",
            Text(label, style=style),
            details["recommended_quant"],
            details["tokens_per_sec"],
            Text(thermal, style=thermal_style),
        )

    console.print(table)
    console.print()


def render_summary(results, info):
    smooth     = [r for r in results if r[2] == "smooth"]
    acceptable = [r for r in results if r[2] == "acceptable"]
    cpu_only   = [r for r in results if r[2] == "cpu_only"]
    lag        = [r for r in results if r[2] == "lag"]
    impossible = [r for r in results if r[2] == "impossible"]

    console.print(Rule("[bold]Summary & Recommendations", style="cyan"))
    console.print()

    lines = []

    if smooth:
        best = smooth[-1][0]  # Largest model that runs smoothly
        lines.append(
            f"  [bold green]{len(smooth)} model(s) will run smoothly.[/bold green]"
            f"  Best pick: [bold]{best['name']}[/bold]"
        )
    if acceptable:
        lines.append(f"  [bold yellow]{len(acceptable)} model(s) are usable with some slowdown.[/bold yellow]")
    if cpu_only:
        lines.append(f"  [bold orange1]{len(cpu_only)} model(s) will run on CPU only (slow but possible).[/bold orange1]")
        lines.append(f"     Tip: Use [bold]llama.cpp[/bold] for the best CPU inference performance.")
    if lag:
        lines.append(f"  [bold dark_orange]{len(lag)} model(s) will lag due to memory pressure.[/bold dark_orange]")
    if impossible:
        lines.append(f"  [bold red]{len(impossible)} model(s) are too large for your hardware.[/bold red]")

    lines.append("")

    # Hardware-specific tips
    if info.get("is_apple_silicon"):
        lines.append("  [bold cyan]Apple Silicon detected![/bold cyan] You benefit from fast unified memory.")
        lines.append("     Use [bold]llama.cpp[/bold] with the Metal backend ([bold]-ngl 1[/bold]) for best results.")

    if info["is_laptop"]:
        lines.append("  [bold red]Laptop detected.[/bold red] Long inference sessions can cause thermal throttling.")
        lines.append("     Plug in to power. Consider a cooling pad for 13B+ models.")

    if info["storage_type"] == "HDD":
        lines.append("  [bold yellow]HDD detected.[/bold yellow] Model load times will be slow (minutes, not seconds).")
        lines.append("     Move your GGUF model files to an SSD if possible.")

    if info.get("multi_gpu"):
        lines.append(
            f"  [bold green]Multi-GPU detected![/bold green] {info['total_vram_gb']} GB total VRAM available."
        )
        lines.append("     Use [bold]llama.cpp --tensor-split[/bold] to distribute across GPUs.")

    lines.append("")

    # Recommended inference backend
    backend = info.get("backend", "")
    if "CUDA" in backend:
        lines.append("  [dim]Recommended runner: [bold]Ollama[/bold] or [bold]llama.cpp[/bold] with CUDA[/dim]")
    elif "Metal" in backend:
        lines.append("  [dim]Recommended runner: [bold]Ollama[/bold] or [bold]llama.cpp[/bold] with Metal (-ngl 1)[/dim]")
    elif "ROCm" in backend:
        lines.append("  [dim]Recommended runner: [bold]llama.cpp[/bold] built with ROCm support[/dim]")
    else:
        lines.append("  [dim]Recommended runner: [bold]llama.cpp[/bold] (CPU mode) or [bold]Ollama[/bold][/dim]")

    lines.append("  [dim]Find GGUF models at: [bold]https://huggingface.co/bartowski[/bold][/dim]")

    console.print(Panel("\n".join(lines), border_style="cyan", padding=(1, 2), title="[bold cyan]Results"))
    console.print()


def render_legend():
    console.print(Rule("[bold]Legend", style="dim"))
    console.print()
    items = [
        ("‚úÖ Smooth",       "Runs well with good tokens/sec"),
        ("‚ö° Acceptable",   "Runs with some noticeable slowdown"),
        ("‚ö†Ô∏è  Lag Likely",  "Runs but slowly due to memory pressure"),
        ("üê¢ CPU Only",    "No usable VRAM ‚Äî will use CPU (slow)"),
        ("‚ùå Won't Run",   "Not enough RAM or VRAM"),
        ("Hot (Thermal)",  "Laptop risk: sustained load may throttle"),
    ]
    for icon, desc in items:
        console.print(f"  {icon:<24} [dim]{desc}[/dim]")
    console.print()


# ---------------------------------------------------------------------------
# JSON export
# ---------------------------------------------------------------------------

def export_results(results, info, output_path):
    """Save the full results to a JSON file for further processing."""
    data = {
        "generated_at": datetime.now().isoformat(),
        "system": {k: v for k, v in info.items() if k != "gpus"},
        "gpus":   info.get("gpus", []),
        "results": [
            {
                "model":        r[0]["name"],
                "provider":     r[0].get("provider", ""),
                "params":       r[0]["params"],
                "status":       r[2],
                "score":        round(r[1], 3),
                "quant":        r[3]["recommended_quant"],
                "tok_per_sec":  r[3]["tokens_per_sec"],
                "thermal_risk": r[3]["thermal_risk"],
            }
            for r in results
        ],
    }
    Path(output_path).write_text(json.dumps(data, indent=2))
    console.print(f"  [green]Results saved to [bold]{output_path}[/bold][/green]")
    console.print()


# ---------------------------------------------------------------------------
# CLI argument parsing
# ---------------------------------------------------------------------------

def parse_args():
    parser = argparse.ArgumentParser(
        description="LLM Compatibility Checker ‚Äî find what AI models run on your PC",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
examples:
  python checker.py                        run full compatibility check
  python checker.py --update               fetch latest models from HuggingFace
  python checker.py --filter coding        show only coding models
  python checker.py --runnable             hide models that won't run
  python checker.py --min-params 7         show models 7B and larger
  python checker.py --max-params 13        show models 13B and smaller
  python checker.py --export results.json  save results to JSON
        """,
    )
    parser.add_argument(
        "--update",
        action="store_true",
        help="Refresh model database from HuggingFace API",
    )
    parser.add_argument(
        "--filter",
        type=str,
        default=None,
        metavar="USE_CASE",
        help="Filter models by use case: general, coding, reasoning, vision, embedding",
    )
    parser.add_argument(
        "--runnable",
        action="store_true",
        help="Only show models that can run on your hardware",
    )
    parser.add_argument(
        "--min-params",
        type=float,
        default=None,
        metavar="B",
        help="Only show models with at least this many billion parameters",
    )
    parser.add_argument(
        "--max-params",
        type=float,
        default=None,
        metavar="B",
        help="Only show models with at most this many billion parameters",
    )
    parser.add_argument(
        "--export",
        type=str,
        default=None,
        metavar="FILE",
        help="Export results to a JSON file",
    )
    parser.add_argument(
        "--no-legend",
        action="store_true",
        help="Skip the legend at the bottom of the output",
    )
    return parser.parse_args()


# ---------------------------------------------------------------------------
# Main entry point
# ---------------------------------------------------------------------------

def main():
    args = parse_args()
    render_header()

    # --update: refresh model database then exit
    if args.update:
        update_model_db()
        console.print("  [dim]Run without --update to check compatibility.[/dim]")
        console.print()
        return

    # Load model list
    console.print()
    all_models = load_models()

    # Apply any user filters
    models = all_models

    if args.filter:
        filter_term = args.filter.lower()
        models = [
            m for m in models
            if filter_term in m.get("use_case", "").lower()
            or filter_term in " ".join(m.get("tags", [])).lower()
        ]
        console.print(f"  [dim]Filter: [bold]{args.filter}[/bold]  ‚Üí  {len(models)} models matched[/dim]")

    if args.min_params is not None:
        models = [m for m in models if m["params"] >= args.min_params]

    if args.max_params is not None:
        models = [m for m in models if m["params"] <= args.max_params]

    console.print()

    # Scan hardware
    info = {}
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(bar_width=38),
        TaskProgressColumn(),
        console=console,
        transient=True,
    ) as progress:
        task = progress.add_task("Scanning hardware...", total=4)

        progress.update(task, description="Detecting CPU and RAM...")
        ram = psutil.virtual_memory()
        info.update({
            "cpu_name":           platform.processor() or "",
            "cpu_cores_physical": psutil.cpu_count(logical=False) or 0,
            "cpu_cores_logical":  psutil.cpu_count(logical=True)  or 0,
            "cpu_freq_ghz":       None,
            "ram_total_gb":       round(ram.total    / (1024 ** 3), 1),
            "ram_available_gb":   round(ram.available / (1024 ** 3), 1),
            "ram_used_pct":       ram.percent,
            "os":                 platform.system(),
            "os_version":         platform.version(),
            "machine":            platform.machine(),
        })
        try:
            freq = psutil.cpu_freq()
            info["cpu_freq_ghz"] = round(freq.max / 1000, 2) if freq and freq.max else None
        except Exception:
            pass
        progress.advance(task, 1)

        progress.update(task, description="Detecting GPU (NVIDIA / AMD / Apple)...")
        info["gpus"]             = []
        info["total_vram_gb"]    = 0
        info["is_apple_silicon"] = False
        info["multi_gpu"]        = False
        _detect_gpus(info)
        apple_vram = _get_apple_silicon_vram(info)
        if apple_vram > 0 and not info["gpus"]:
            info["gpus"] = [{
                "name":    info["cpu_name"],
                "vram_gb": apple_vram,
                "vendor":  "Apple (Unified Memory)",
                "driver":  "Metal",
                "load":    None,
                "temp":    None,
            }]
            info["total_vram_gb"]    = apple_vram
            info["is_apple_silicon"] = True
        if len(info["gpus"]) > 1:
            info["total_vram_gb"] = sum(g["vram_gb"] for g in info["gpus"])
            info["multi_gpu"]     = True
        progress.advance(task, 1)

        progress.update(task, description="Detecting storage and form factor...")
        info["storage_type"] = _detect_storage_type()
        info["is_laptop"]    = _detect_laptop()
        info["backend"]      = detect_backend(info)
        progress.advance(task, 1)

        progress.update(task, description="Scoring models...")
        progress.advance(task, 1)

    # Fix CPU name if platform.processor() returned empty (common on Linux/macOS)
    if not info["cpu_name"].strip():
        try:
            if info["os"] == "Darwin":
                info["cpu_name"] = subprocess.check_output(
                    ["sysctl", "-n", "machdep.cpu.brand_string"],
                    stderr=subprocess.DEVNULL,
                ).decode().strip()
            elif info["os"] == "Linux":
                for line in subprocess.check_output(
                    ["lscpu"], stderr=subprocess.DEVNULL
                ).decode().splitlines():
                    if "Model name" in line:
                        info["cpu_name"] = line.split(":")[1].strip()
                        break
        except Exception:
            info["cpu_name"] = "Unknown CPU"

    # Score every model
    results = []
    for model in models:
        score, status, details = score_model(model, info)
        results.append((model, score, status, details))

    # Filter to runnable only if requested
    if args.runnable:
        results = [r for r in results if r[2] != "impossible"]

    # Sort: best status first, then by model size descending within each tier
    status_order = {"smooth": 0, "acceptable": 1, "lag": 2, "cpu_only": 3, "impossible": 4}
    results.sort(key=lambda r: (status_order.get(r[2], 5), -r[0]["params"]))

    # Render output
    render_system_info(info)
    render_results_table(results)
    render_summary(results, info)
    if not args.no_legend:
        render_legend()

    # Export if requested
    if args.export:
        export_results(results, info, args.export)


if __name__ == "__main__":
    main()
